{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Patch Assignment — ysais_adversarial_patch_full\n",
    "\n",
    "**Author:** Ysais Martinez\n",
    "\n",
    "**Purpose:** Create, optimize, and test adversarial patches on ResNet34 (ImageNet). \n",
    "Designed to meet the rubric: ≥5 before/after visuals, reproducible code, creativity/disguise, and a short reflection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ====== Environment setup (Colab-friendly) ======\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# If running in Colab, uncomment the pip install line. If running locally, ensure these packages are installed.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# !pip install -q torch torchvision timm matplotlib opencv-python-headless\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m); np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m); random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      7\u001b[0m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39mdeterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# ====== Environment setup (Colab-friendly) ======\n",
    "# If running in Colab, uncomment the pip install line. If running locally, ensure these packages are installed.\n",
    "!pip install -q torch torchvision timm matplotlib opencv-python-headless\n",
    "\n",
    "import torch, random, numpy as np\n",
    "torch.manual_seed(42); np.random.seed(42); random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, cv2, os, pathlib, sys\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files needed\n",
    "Place the following in the same directory as this notebook:\n",
    "- `imagenet_classes.txt` (ImageNet label names, one per line)\n",
    "- `data/` folder containing **≥ 5 images** you will test (jpg/png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "# Use the ImageNet-pretrained ResNet34 (assignment requirement)\n",
    "try:\n",
    "    model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "except Exception:\n",
    "    model = models.resnet34(pretrained=True)\n",
    "\n",
    "model = model.to(device).eval()\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "print(\"Loaded ResNet34 (eval mode)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2label = None\n",
    "if os.path.exists('imagenet_classes.txt'):\n",
    "    with open('imagenet_classes.txt','r') as f:\n",
    "        idx2label = [x.strip() for x in f]\n",
    "    print('Loaded imagenet_classes.txt with', len(idx2label), 'labels')\n",
    "else:\n",
    "    print('Warning: imagenet_classes.txt not found. Predictions will show indices only.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms & helpers\n",
    "preprocess = T.Compose([\n",
    "    T.Resize((224,224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def deprocess(t):\n",
    "    arr = t.squeeze(0).cpu().numpy().transpose(1,2,0)\n",
    "    arr = (arr * np.array([0.229,0.224,0.225])) + np.array([0.485,0.456,0.406])\n",
    "    arr = np.clip(arr * 255.0, 0, 255).astype(np.uint8)\n",
    "    return arr\n",
    "\n",
    "def load_img(path):\n",
    "    return Image.open(path).convert('RGB')\n",
    "\n",
    "def image_to_tensor(img_pil):\n",
    "    return preprocess(img_pil).unsqueeze(0).to(device)\n",
    "\n",
    "def apply_patch_to_numpy(img_np, patch_np, top_left=(10,10)):\n",
    "    img = img_np.copy()\n",
    "    ph, pw = patch_np.shape[:2]\n",
    "    x,y = top_left\n",
    "    h,w = img.shape[:2]\n",
    "    ph = min(ph, h-y)\n",
    "    pw = min(pw, w-x)\n",
    "    if ph <=0 or pw <=0:\n",
    "        return img\n",
    "    img[y:y+ph, x:x+pw] = patch_np[:ph, :pw]\n",
    "    return img\n",
    "\n",
    "def predict_and_print(img_pil):\n",
    "    x = image_to_tensor(img_pil)\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "        probs = softmax(out)\n",
    "        top1_prob, top1_idx = probs.max(1)\n",
    "    label = str(top1_idx.item())\n",
    "    if idx2label is not None:\n",
    "        label = f\"{top1_idx.item()} - {idx2label[top1_idx.item()]}\"\n",
    "    return top1_idx.item(), float(top1_prob.item()), label\n",
    "\n",
    "print('Helpers defined.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Simple adversarial patch optimization (toy example)\n",
    "\n",
    "**Goal:** produce a patch that increases the model's probability for a chosen **target class** when pasted onto images.  \n",
    "This is an educational implementation; cite Brown et al. (2017) for the canonical method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Parameters (edit these) ----\n",
    "target_class = 859  # change to target ImageNet ID you want to force (see imagenet_classes.txt)\n",
    "ph, pw = 60, 60     # patch size (pixels)\n",
    "lr = 0.2\n",
    "epochs = 30\n",
    "\n",
    "# Optimizable patch tensor\n",
    "patch = torch.randn(3, ph, pw, device=device, requires_grad=True)\n",
    "optimizer = torch.optim.Adam([patch], lr=lr)\n",
    "\n",
    "# Collect training images from data/\n",
    "train_images = []\n",
    "data_dir = 'data'\n",
    "if os.path.exists(data_dir):\n",
    "    for p in sorted(os.listdir(data_dir)):\n",
    "        if p.lower().endswith(('.jpg','.jpeg','.png')):\n",
    "            train_images.append(os.path.join(data_dir,p))\n",
    "print('Found', len(train_images), 'images in data/ (for patch optimization)')\n",
    "\n",
    "if len(train_images) == 0:\n",
    "    print('No training images found. Upload images to the data/ folder and re-run.')\n",
    "else:\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for pth in train_images:\n",
    "            pil = load_img(pth).resize((224,224))\n",
    "            x = image_to_tensor(pil)\n",
    "\n",
    "            # Random top-left location\n",
    "            y = np.random.randint(0, 224-ph)\n",
    "            xcoord = np.random.randint(0, 224-pw)\n",
    "\n",
    "            # Build patched image\n",
    "            img_np = deprocess(x)\n",
    "            patch_np = (torch.sigmoid(patch).detach().cpu().numpy().transpose(1,2,0) * 255).astype(np.uint8)\n",
    "            patched_np = apply_patch_to_numpy(img_np, patch_np, top_left=(xcoord,y))\n",
    "            patched_pil = Image.fromarray(patched_np)\n",
    "            x_patched = image_to_tensor(patched_pil)\n",
    "\n",
    "            # Increase probability of target class\n",
    "            out = model(x_patched)\n",
    "            loss = - torch.nn.functional.log_softmax(out, dim=1)[:, target_class].mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += float(loss.item())\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs}  avg_loss: {total_loss/len(train_images):.4f}\")\n",
    "\n",
    "    final_patch_np = (torch.sigmoid(patch).detach().cpu().numpy().transpose(1,2,0) * 255).astype(np.uint8)\n",
    "    Image.fromarray(final_patch_np).save('final_patch.png')\n",
    "    print('Saved final_patch.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Visualize results on 5 example images\n",
    "\n",
    "Iterate over 5 images, apply the optimized patch, show side-by-side before/after, print predictions, and save results to `results/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 5 example images\n",
    "examples = []\n",
    "if os.path.exists('data'):\n",
    "    for p in sorted(os.listdir('data')):\n",
    "        if p.lower().endswith(('.jpg','.jpeg','.png')):\n",
    "            examples.append(os.path.join('data',p))\n",
    "examples = examples[:5]\n",
    "\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "from matplotlib import gridspec\n",
    "if len(examples) == 0:\n",
    "    print('No example images found in data/. Please add 5 images and re-run.')\n",
    "else:\n",
    "    for i, pth in enumerate(examples):\n",
    "        pil = load_img(pth).resize((224,224))\n",
    "        orig_idx, orig_prob, orig_lbl = predict_and_print(pil)\n",
    "\n",
    "        # Apply patch at a fixed location for demo\n",
    "        patched_np = apply_patch_to_numpy(np.array(pil), final_patch_np, top_left=(20,20))\n",
    "        patched_pil = Image.fromarray(patched_np)\n",
    "        patched_idx, patched_prob, patched_lbl = predict_and_print(patched_pil)\n",
    "\n",
    "        fig = plt.figure(figsize=(8,4))\n",
    "        gs = gridspec.GridSpec(1,2, width_ratios=[1,1])\n",
    "        ax0 = plt.subplot(gs[0]); ax0.imshow(pil); ax0.set_title(f\"Orig: {orig_lbl}\\n{orig_prob:.3f}\"); ax0.axis('off')\n",
    "        ax1 = plt.subplot(gs[1]); ax1.imshow(patched_pil); ax1.set_title(f\"Patched: {patched_lbl}\\n{patched_prob:.3f}\"); ax1.axis('off')\n",
    "        plt.tight_layout()\n",
    "        outpath = os.path.join('results', f'example_{i+1}.png')\n",
    "        plt.savefig(outpath, dpi=200, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print('Saved:', outpath)\n",
    "\n",
    "    print('\\nAll visuals saved to results/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Creativity / Disguise ideas\n",
    "\n",
    "- Paste `final_patch.png` onto an image of a **phone, backpack, or clothing** to simulate a sticker placement. Save the composite.\n",
    "- Try **two patches** at different positions and re-run predictions.\n",
    "- For **physical demo**: print `final_patch.png` at 100% scale on sticker paper (color) and bring to class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Reflection (paste your write-up below)\n",
    "Paste your human-sounding reflection about what the model focused on, surprising/misleading examples, and why explainability matters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. What visual cues did the model focus on\n",
    "\n",
    "Overall, the model tended to pay the most attention to **object shapes and contours** — things like bottle necks, rims, and bin openings. **GradCAM++** usually produced the sharpest focus, often outlining the exact recyclable object.\n",
    "\n",
    "At the same time, some of the heatmaps, especially from **ScoreCAM** and the base **GradCAM**, locked onto **brand logos or text** rather than the actual material. It’s easy to see why — logos are bright and high-contrast — but they don’t tell you if something is plastic, glass, or metal.\n",
    "\n",
    "**EigenCAM** behaved differently. It gave a broader view of the whole scene, which helped show the model’s overall sense of *context*, but it also made the attention more diffuse. In cluttered images, that global approach sometimes caused the model to highlight the background instead of the items we actually care about.\n",
    "\n",
    "### B. Surprising or misleading examples\n",
    "\n",
    "- **Magazine cover:** This one really stood out. Every method focused on the *person’s face and text* instead of the paper material. It shows how easily the model can get distracted by visually dominant features that have nothing to do with recyclability.\n",
    "\n",
    "- **Bottles close-up:** This was a good example. **GradCAM++** zeroed in on the *bottle edges and caps*, which are meaningful visual cues for identifying plastic.\n",
    "\n",
    "- **Cluttered bin scene:** The model did a decent job picking one main bottle but ignored the other items. It’s doing fine on the *top-1* target but not capturing the full scene — something to watch out for if we ever want multi-object predictions.\n",
    "\n",
    "- **Chairs image:** **EigenCAM** spilled into the background quite a bit. It reminded me that global attention methods can lose precision when objects have thin edges or narrow legs.\n",
    "\n",
    "### C. Why explainability matters here\n",
    "\n",
    "For **recycling classification**, explainability is essential. Cities and facilities care about the *material*, not the logo or the lighting. Seeing where the model looks helps confirm it’s learning the right thing.\n",
    "\n",
    "The heatmaps also made it clear where the dataset could improve — too many clean product photos, not enough messy, real-world bins. By adding more variety and clutter, the model could learn *material cues* instead of shortcuts like text or brand color.\n",
    "\n",
    "Finally, explainability matters for **trust**. If a recycling AI is used in the real world, operators need to see *why* an item was classified a certain way. Transparent reasoning makes it possible to fix mistakes and show accountability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Submission checklist\n",
    "\n",
    "- [ ] Notebook runs top-to-bottom in Colab (add files to `data/` and `imagenet_classes.txt`).\n",
    "- [ ] `results/` contains 5 `example_*.png` images showing before/after and predictions.\n",
    "- [ ] `final_patch.png` saved and ready to print (100% scale recommended).\n",
    "- [ ] README in repo with run instructions and a short description of the creative patch design.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
